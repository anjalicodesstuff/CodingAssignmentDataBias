# CodingAssignmentDataBias
Coding Assignment: Data Bias, I310D


Step 3: Testing

Below are a few of my hypotheses: 

The Perspective API will detect toxicity more accurately in shorter statements.
The Perspective API will be less likely to find toxicity in anti-male comments over anti-female comments.
The Perspective API will find comments with curse words more toxic. 

Step 4: Results & Analysis

What biases do you think might exist in the model based on intuitions or public documentation about how the model was created?
I think this model may be biased in terms of measuring what phrases/words are considered toxic. 
What were your results?
I found that the shorter the statements, it was easier for the model to understand/detect toxicity. Howevevr, longer comments with more complex insults/phrases tended to be scored lower in terms of toxicity than I would have found. 
What theories do you have about why your results are what they are?
I believe this is because the model may not be familiar with the sort of phrases that are newer or take more context to measure something like toxicity. 
